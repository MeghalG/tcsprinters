{"_ts": 1761543521.191904, "data": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3DDueling%20optimization%20with%20a%20monotone%20adversary%26id_list%3D%26start%3D0%26max_results%3D25\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=Dueling optimization with a monotone adversary&amp;id_list=&amp;start=0&amp;max_results=25</title>\n  <id>http://arxiv.org/api/iNjOUKo9Sx/GJvMpFvKhZI7aHg0</id>\n  <updated>2025-10-27T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2789401</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">25</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/2202.06694v1</id>\n    <updated>2022-02-14T13:37:23Z</updated>\n    <published>2022-02-14T13:37:23Z</published>\n    <title>Versatile Dueling Bandits: Best-of-both-World Analyses for Online\n  Learning from Preferences</title>\n    <summary>  We study the problem of $K$-armed dueling bandit for both stochastic and\nadversarial environments, where the goal of the learner is to aggregate\ninformation through relative preferences of pair of decisions points queried in\nan online sequential manner. We first propose a novel reduction from any\n(general) dueling bandits to multi-armed bandits and despite the simplicity, it\nallows us to improve many existing results in dueling bandits. In particular,\n\\emph{we give the first best-of-both world result for the dueling bandits\nregret minimization problem} -- a unified framework that is guaranteed to\nperform optimally for both stochastic and adversarial preferences\nsimultaneously. Moreover, our algorithm is also the first to achieve an optimal\n$O(\\sum_{i = 1}^K \\frac{\\log T}{\\Delta_i})$ regret bound against the\nCondorcet-winner benchmark, which scales optimally both in terms of the\narm-size $K$ and the instance-specific suboptimality gaps $\\{\\Delta_i\\}_{i =\n1}^K$. This resolves the long-standing problem of designing an instancewise\ngap-dependent order optimal regret algorithm for dueling bandits (with matching\nlower bounds up to small constant factors). We further justify the robustness\nof our proposed algorithm by proving its optimal regret rate under\nadversarially corrupted preferences -- this outperforms the existing\nstate-of-the-art corrupted dueling results by a large margin. In summary, we\nbelieve our reduction idea will find a broader scope in solving a diverse class\nof dueling bandits setting, which are otherwise studied separately from\nmulti-armed bandits with often more complex solutions and worse guarantees. The\nefficacy of our proposed algorithms is empirically corroborated against the\nexisting dueling bandit methods.\n</summary>\n    <author>\n      <name>Aadirupa Saha</name>\n    </author>\n    <author>\n      <name>Pierre Gaillard</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2202.06694v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2202.06694v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2311.11185v1</id>\n    <updated>2023-11-18T23:55:59Z</updated>\n    <published>2023-11-18T23:55:59Z</published>\n    <title>Dueling Optimization with a Monotone Adversary</title>\n    <summary>  We introduce and study the problem of dueling optimization with a monotone\nadversary, which is a generalization of (noiseless) dueling convex\noptimization. The goal is to design an online algorithm to find a minimizer\n$\\mathbf{x}^{*}$ for a function $f\\colon X \\to \\mathbb{R}$, where $X \\subseteq\n\\mathbb{R}^d$. In each round, the algorithm submits a pair of guesses, i.e.,\n$\\mathbf{x}^{(1)}$ and $\\mathbf{x}^{(2)}$, and the adversary responds with any\npoint in the space that is at least as good as both guesses. The cost of each\nquery is the suboptimality of the worse of the two guesses; i.e., ${\\max}\n\\left( f(\\mathbf{x}^{(1)}), f(\\mathbf{x}^{(2)}) \\right) - f(\\mathbf{x}^{*})$.\nThe goal is to minimize the number of iterations required to find an\n$\\varepsilon$-optimal point and to minimize the total cost (regret) of the\nguesses over many rounds. Our main result is an efficient randomized algorithm\nfor several natural choices of the function $f$ and set $X$ that incurs cost\n$O(d)$ and iteration complexity $O(d\\log(1/\\varepsilon)^2)$. Moreover, our\ndependence on $d$ is asymptotically optimal, as we show examples in which any\nrandomized algorithm for this problem must incur $\\Omega(d)$ cost and iteration\ncomplexity.\n</summary>\n    <author>\n      <name>Avrim Blum</name>\n    </author>\n    <author>\n      <name>Meghal Gupta</name>\n    </author>\n    <author>\n      <name>Gene Li</name>\n    </author>\n    <author>\n      <name>Naren Sarayu Manoj</name>\n    </author>\n    <author>\n      <name>Aadirupa Saha</name>\n    </author>\n    <author>\n      <name>Yuanyuan Yang</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">21 pages. comments welcome</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2311.11185v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2311.11185v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.DS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2105.11802v2</id>\n    <updated>2021-06-09T07:04:02Z</updated>\n    <published>2021-05-25T10:08:41Z</published>\n    <title>Bias-Robust Bayesian Optimization via Dueling Bandits</title>\n    <summary>  We consider Bayesian optimization in settings where observations can be\nadversarially biased, for example by an uncontrolled hidden confounder. Our\nfirst contribution is a reduction of the confounded setting to the dueling\nbandit model. Then we propose a novel approach for dueling bandits based on\ninformation-directed sampling (IDS). Thereby, we obtain the first efficient\nkernelized algorithm for dueling bandits that comes with cumulative regret\nguarantees. Our analysis further generalizes a previously proposed\nsemi-parametric linear bandit model to non-linear reward functions, and\nuncovers interesting links to doubly-robust estimation.\n</summary>\n    <author>\n      <name>Johannes Kirschner</name>\n    </author>\n    <author>\n      <name>Andreas Krause</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2105.11802v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2105.11802v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2404.10776v2</id>\n    <updated>2025-02-28T18:56:33Z</updated>\n    <published>2024-04-16T17:59:55Z</published>\n    <title>Nearly Optimal Algorithms for Contextual Dueling Bandits from\n  Adversarial Feedback</title>\n    <summary>  Learning from human feedback plays an important role in aligning generative\nmodels, such as large language models (LLM). However, the effectiveness of this\napproach can be influenced by adversaries, who may intentionally provide\nmisleading preferences to manipulate the output in an undesirable or harmful\ndirection. To tackle this challenge, we study a specific model within this\nproblem domain--contextual dueling bandits with adversarial feedback, where the\ntrue preference label can be flipped by an adversary. We propose an algorithm\nnamely robust contextual dueling bandits (RCDB), which is based on\nuncertainty-weighted maximum likelihood estimation. Our algorithm achieves an\n$\\tilde O(d\\sqrt{T}/\\kappa+dC/\\kappa)$ regret bound, where $T$ is the number of\nrounds, $d$ is the dimension of the context, $\\kappa$ is the lower bound of the\nderivative of the link function, and $ 0 \\le C \\le T$ is the total number of\nadversarial feedback. We also prove a lower bound to show that our regret bound\nis nearly optimal, both in scenarios with and without ($C=0$) adversarial\nfeedback. Our work is the first to achieve nearly minimax optimal regret for\ndueling bandits in the presence of adversarial preference feedback.\nAdditionally, for the sigmoid link function, we develop a novel algorithm that\ntakes into account the effect of local derivatives into maximum likelihood\nestimation (MLE) analysis through a refined method for estimating the link\nfunction's derivative. This method helps us to eliminate the $\\kappa$\ndependence in the leading term with respect to $T$, which reduces the\nexponential dependence on the parameter radius $B$ to a polynomial dependence.\n</summary>\n    <author>\n      <name>Qiwei Di</name>\n    </author>\n    <author>\n      <name>Jiafan He</name>\n    </author>\n    <author>\n      <name>Quanquan Gu</name>\n    </author>\n    <arxiv:comment xmlns:arxiv=\"http://arxiv.org/schemas/atom\">44pages, 2 figures, 1 table</arxiv:comment>\n    <link href=\"http://arxiv.org/abs/2404.10776v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2404.10776v2\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2211.10293v1</id>\n    <updated>2022-11-16T22:00:54Z</updated>\n    <published>2022-11-16T22:00:54Z</published>\n    <title>Dueling Bandits: From Two-dueling to Multi-dueling</title>\n    <summary>  We study a general multi-dueling bandit problem, where an agent compares\nmultiple options simultaneously and aims to minimize the regret due to\nselecting suboptimal arms. This setting generalizes the traditional two-dueling\nbandit problem and finds many real-world applications involving subjective\nfeedback on multiple options. We start with the two-dueling bandit setting and\npropose two efficient algorithms, DoublerBAI and MultiSBM-Feedback. DoublerBAI\nprovides a generic schema for translating known results on best arm\nidentification algorithms to the dueling bandit problem, and achieves a regret\nbound of $O(\\ln T)$. MultiSBM-Feedback not only has an optimal $O(\\ln T)$\nregret, but also reduces the constant factor by almost a half compared to\nbenchmark results. Then, we consider the general multi-dueling case and develop\nan efficient algorithm MultiRUCB. Using a novel finite-time regret analysis for\nthe general multi-dueling bandit problem, we show that MultiRUCB also achieves\nan $O(\\ln T)$ regret bound and the bound tightens as the capacity of the\ncomparison set increases. Based on both synthetic and real-world datasets, we\nempirically demonstrate that our algorithms outperform existing algorithms.\n</summary>\n    <author>\n      <name>Yihan Du</name>\n    </author>\n    <author>\n      <name>Siwei Wang</name>\n    </author>\n    <author>\n      <name>Longbo Huang</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/2211.10293v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/2211.10293v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"}