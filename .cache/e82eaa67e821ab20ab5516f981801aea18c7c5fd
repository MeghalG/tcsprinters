{"_ts": 1761545807.415838, "data": "{\"status\": \"ok\", \"message-type\": \"work-list\", \"message-version\": \"1.0.0\", \"message\": {\"facets\": {}, \"total-results\": 3610093, \"items\": [{\"indexed\": {\"date-parts\": [[2023, 5, 2]], \"date-time\": \"2023-05-02T04:21:39Z\", \"timestamp\": 1683001299312}, \"posted\": {\"date-parts\": [[2023]]}, \"group-title\": \"SSRN\", \"reference-count\": 0, \"publisher\": \"Elsevier BV\", \"content-domain\": {\"domain\": [], \"crossmark-restriction\": false}, \"DOI\": \"10.2139/ssrn.4431992\", \"type\": \"posted-content\", \"created\": {\"date-parts\": [[2023, 4, 28]], \"date-time\": \"2023-04-28T13:20:11Z\", \"timestamp\": 1682688011000}, \"source\": \"Crossref\", \"is-referenced-by-count\": 0, \"title\": [\"Learning from Different Perspectives for Regret Reduction in Reinforcement Learning: A Free Energy Approach\"], \"prefix\": \"10.2139\", \"author\": [{\"given\": \"Milad\", \"family\": \"Ghorbani\", \"sequence\": \"first\", \"affiliation\": []}, {\"given\": \"Reshad\", \"family\": \"Hosseini\", \"sequence\": \"additional\", \"affiliation\": []}, {\"given\": \"Seyed Pooya\", \"family\": \"Shariatpanahi\", \"sequence\": \"additional\", \"affiliation\": []}, {\"given\": \"Majid\", \"family\": \"Nili Ahmadabadi\", \"sequence\": \"additional\", \"affiliation\": []}], \"member\": \"78\", \"deposited\": {\"date-parts\": [[2023, 5, 1]], \"date-time\": \"2023-05-01T18:56:37Z\", \"timestamp\": 1682967397000}, \"score\": 37.748894, \"resource\": {\"primary\": {\"URL\": \"https://www.ssrn.com/abstract=4431992\"}}, \"issued\": {\"date-parts\": [[2023]]}, \"references-count\": 0, \"URL\": \"https://doi.org/10.2139/ssrn.4431992\", \"published\": {\"date-parts\": [[2023]]}, \"subtype\": \"preprint\"}, {\"indexed\": {\"date-parts\": [[2025, 5, 13]], \"date-time\": \"2025-05-13T20:29:25Z\", \"timestamp\": 1747168165035, \"version\": \"3.40.5\"}, \"reference-count\": 79, \"publisher\": \"Elsevier BV\", \"license\": [{\"start\": {\"date-parts\": [[2025, 1, 1]], \"date-time\": \"2025-01-01T00:00:00Z\", \"timestamp\": 1735689600000}, \"content-version\": \"tdm\", \"delay-in-days\": 0, \"URL\": \"https://www.elsevier.com/tdm/userlicense/1.0/\"}, {\"start\": {\"date-parts\": [[2025, 1, 1]], \"date-time\": \"2025-01-01T00:00:00Z\", \"timestamp\": 1735689600000}, \"content-version\": \"tdm\", \"delay-in-days\": 0, \"URL\": \"https://www.elsevier.com/legal/tdmrep-license\"}, {\"start\": {\"date-parts\": [[2025, 1, 1]], \"date-time\": \"2025-01-01T00:00:00Z\", \"timestamp\": 1735689600000}, \"content-version\": \"stm-asf\", \"delay-in-days\": 0, \"URL\": \"https://doi.org/10.15223/policy-017\"}, {\"start\": {\"date-parts\": [[2025, 1, 1]], \"date-time\": \"2025-01-01T00:00:00Z\", \"timestamp\": 1735689600000}, \"content-version\": \"stm-asf\", \"delay-in-days\": 0, \"URL\": \"https://doi.org/10.15223/policy-037\"}, {\"start\": {\"date-parts\": [[2025, 1, 1]], \"date-time\": \"2025-01-01T00:00:00Z\", \"timestamp\": 1735689600000}, \"content-version\": \"stm-asf\", \"delay-in-days\": 0, \"URL\": \"https://doi.org/10.15223/policy-012\"}, {\"start\": {\"date-parts\": [[2025, 1, 1]], \"date-time\": \"2025-01-01T00:00:00Z\", \"timestamp\": 1735689600000}, \"content-version\": \"stm-asf\", \"delay-in-days\": 0, \"URL\": \"https://doi.org/10.15223/policy-029\"}, {\"start\": {\"date-parts\": [[2025, 1, 1]], \"date-time\": \"2025-01-01T00:00:00Z\", \"timestamp\": 1735689600000}, \"content-version\": \"stm-asf\", \"delay-in-days\": 0, \"URL\": \"https://doi.org/10.15223/policy-004\"}], \"content-domain\": {\"domain\": [\"elsevier.com\", \"sciencedirect.com\"], \"crossmark-restriction\": true}, \"short-container-title\": [\"Neurocomputing\"], \"published-print\": {\"date-parts\": [[2025, 1]]}, \"DOI\": \"10.1016/j.neucom.2024.128797\", \"type\": \"journal-article\", \"created\": {\"date-parts\": [[2024, 10, 30]], \"date-time\": \"2024-10-30T20:06:32Z\", \"timestamp\": 1730318792000}, \"page\": \"128797\", \"update-policy\": \"https://doi.org/10.1016/elsevier_cm_policy\", \"source\": \"Crossref\", \"is-referenced-by-count\": 0, \"special_numbering\": \"C\", \"title\": [\"Learning from different perspectives for regret reduction in reinforcement learning: A free energy approach\"], \"prefix\": \"10.1016\", \"volume\": \"614\", \"author\": [{\"ORCID\": \"https://orcid.org/0000-0001-7784-8374\", \"authenticated-orcid\": false, \"given\": \"Milad\", \"family\": \"Ghorbani\", \"sequence\": \"first\", \"affiliation\": []}, {\"given\": \"Reshad\", \"family\": \"Hosseini\", \"sequence\": \"additional\", \"affiliation\": []}, {\"given\": \"Seyed Pooya\", \"family\": \"Shariatpanahi\", \"sequence\": \"additional\", \"affiliation\": []}, {\"given\": \"Majid Nili\", \"family\": \"Ahmadabadi\", \"sequence\": \"additional\", \"affiliation\": []}], \"member\": \"78\", \"reference\": [{\"key\": \"10.1016/j.neucom.2024.128797_b1\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"237\", \"DOI\": \"10.1613/jair.301\", \"article-title\": \"Reinforcement learning: A survey\", \"volume\": \"4\", \"author\": \"Kaelbling\", \"year\": \"1996\", \"journal-title\": \"J. Artificial Intelligence Res.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b2\", \"series-title\": \"Advances in Neural Information Processing Systems\", \"article-title\": \"Learning vehicular dynamics, with application to modeling helicopters\", \"author\": \"Abbeel\", \"year\": \"2006\"}, {\"issue\": \"11\", \"key\": \"10.1016/j.neucom.2024.128797_b3\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1400\", \"DOI\": \"10.1016/j.robot.2012.05.019\", \"article-title\": \"Real-world reinforcement learning for autonomous humanoid robot docking\", \"volume\": \"60\", \"author\": \"Navarro-Guerrero\", \"year\": \"2012\", \"journal-title\": \"Robot. Auton. Syst.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b4\", \"series-title\": \"14th International Conference on Control, Automation, Robotics and Vision\", \"article-title\": \"Autonomous navigation of UAV by using real-time model-based reinforcement learning\", \"author\": \"Imanberdiyev\", \"year\": \"2016\"}, {\"issue\": \"3\", \"key\": \"10.1016/j.neucom.2024.128797_b5\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"706\", \"DOI\": \"10.1109/TCYB.2015.2414277\", \"article-title\": \"Learning trajectories for robot programing by demonstration using a coordinated mixture of factor analyzers\", \"volume\": \"46\", \"author\": \"Field\", \"year\": \"2016\", \"journal-title\": \"IEEE Trans. Cybern.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b6\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"12\", \"DOI\": \"10.1016/j.dss.2015.03.008\", \"article-title\": \"Recommender system application developments: a survey\", \"volume\": \"74\", \"author\": \"Lu\", \"year\": \"2015\", \"journal-title\": \"Decis. Support Syst.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b7\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"204\", \"DOI\": \"10.1016/j.eswa.2015.12.050\", \"article-title\": \"An evolutionary approach for combining results of recommender systems techniques based on collaborative filtering\", \"volume\": \"53\", \"author\": \"da Silva\", \"year\": \"2016\", \"journal-title\": \"Expert Syst. Appl.\"}, {\"issue\": \"2\", \"key\": \"10.1016/j.neucom.2024.128797_b8\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"119\", \"DOI\": \"10.1007/s40708-016-0042-6\", \"article-title\": \"Interactive machine learning for health informatics: When do we need the human-in-the-loop?\", \"volume\": \"3\", \"author\": \"Holzinger\", \"year\": \"2016\", \"journal-title\": \"Brain Inform.\"}, {\"issue\": \"6344\", \"key\": \"10.1016/j.neucom.2024.128797_b9\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1280\", \"DOI\": \"10.1126/science.aal5054\", \"article-title\": \"Human-in-the-loop optimization of exoskeleton assistance during walking\", \"volume\": \"356\", \"author\": \"Zhang\", \"year\": \"2017\", \"journal-title\": \"Science\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b10\", \"unstructured\": \"J. Li, A.H. Miller, S. Chopra, M. Ranzato, J. Weston, Dialogue Learning with Human-In-The-Loop, in: 5th International Conference on Learning Representations, 2017.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b11\", \"series-title\": \"Advances in Neural Information Processing Systems\", \"article-title\": \"Generalization in reinforcement learning: Successful examples using sparse coarse coding\", \"author\": \"Sutton\", \"year\": \"1996\"}, {\"issue\": \"7540\", \"key\": \"10.1016/j.neucom.2024.128797_b12\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"529\", \"DOI\": \"10.1038/nature14236\", \"article-title\": \"Human-level control through deep reinforcement learning\", \"volume\": \"518\", \"author\": \"Mnih\", \"year\": \"2015\", \"journal-title\": \"Nature\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b13\", \"doi-asserted-by\": \"crossref\", \"unstructured\": \"H. van Hasselt, A. Guez, D. Silver, Deep Reinforcement Learning with Double Q-Learning, in: AAAI Conference on Artificial Intelligence, 2016.\", \"DOI\": \"10.1609/aaai.v30i1.10295\"}, {\"issue\": \"2\", \"key\": \"10.1016/j.neucom.2024.128797_b14\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1007/s10458-021-09497-8\", \"article-title\": \"Playing Atari with few neurons: Improving the efficacy of reinforcement learning by decoupling feature extraction and decision making\", \"volume\": \"35\", \"author\": \"Cuccu\", \"year\": \"2021\", \"journal-title\": \"Auton. Agents Multi-Agent Syst.\"}, {\"issue\": \"12\", \"key\": \"10.1016/j.neucom.2024.128797_b15\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"2613\", \"DOI\": \"10.1109/TCYB.2014.2311578\", \"article-title\": \"A clustering-based graph Laplacian framework for value function approximation in reinforcement learning\", \"volume\": \"44\", \"author\": \"Xu\", \"year\": \"2014\", \"journal-title\": \"IEEE Trans. Cybern.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b16\", \"unstructured\": \"T. Mandel, Y.-E. Liu, E. Brunskill, Z. Popovic, Efficient Bayesian Clustering for Reinforcement Learning., in: International Joint Conference on Artificial Intelligence, 2016.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b17\", \"unstructured\": \"D. Abel, D.E. Hershkowitz, M.L. Littman, Near Optimal Behavior via Approximate State Abstraction, in: 33rd International Conference on Machine Learning, 2016.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b18\", \"unstructured\": \"D. Abel, D. Arumugam, L. Lehnert, M. Littman, State Abstractions for Lifelong Reinforcement Learning, in: 35th International Conference on Machine Learning, 2018.\"}, {\"issue\": \"8\", \"key\": \"10.1016/j.neucom.2024.128797_b19\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1414\", \"DOI\": \"10.1109/TCYB.2014.2352038\", \"article-title\": \"Toward generalization of automated temporal abstraction to partially observable reinforcement learning\", \"volume\": \"45\", \"author\": \"\\u00c7.ilden\", \"year\": \"2015\", \"journal-title\": \"IEEE Trans. Cybern.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b20\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"193\", \"DOI\": \"10.1016/j.neunet.2021.02.026\", \"article-title\": \"Diversity-driven knowledge distillation for financial trading using deep reinforcement learning\", \"volume\": \"140\", \"author\": \"Tsantekidis\", \"year\": \"2021\", \"journal-title\": \"Neural Netw.\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b21\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1007/s10458-019-09430-0\", \"article-title\": \"Agents teaching agents: A survey on inter-agent transfer learning\", \"volume\": \"34\", \"author\": \"Da Silva\", \"year\": \"2020\", \"journal-title\": \"Auton. Agents Multi-Agent Syst.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b22\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1155/2014/428567\", \"article-title\": \"Context transfer in reinforcement learning using action-value functions\", \"author\": \"Mousavi\", \"year\": \"2014\", \"journal-title\": \"Comput. Intell. Neurosci.\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b23\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"77\", \"DOI\": \"10.1109/TCYB.2014.2319733\", \"article-title\": \"Stochastic abstract policies: Generalizing knowledge to improve reinforcement learning\", \"volume\": \"45\", \"author\": \"Koga\", \"year\": \"2015\", \"journal-title\": \"IEEE Trans. Cybern.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b24\", \"unstructured\": \"S. Narvekar, P. Stone, Learning Curriculum Policies for Reinforcement Learning, in: 18th International Conference on Autonomous Agents and MultiAgent Systems, 2019.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b25\", \"article-title\": \"Curriculum learning for reinforcement learning domains: A framework and survey\", \"volume\": \"21\", \"author\": \"Narvekar\", \"year\": \"2020\", \"journal-title\": \"J. Mach. Learn. Res.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b26\", \"unstructured\": \"F.L.D. Silva, A.H.R. Costa, Object-Oriented Curriculum Generation for Reinforcement Learning, in: 17th International Conference on Autonomous Agents and MultiAgent Systems, 2018, pp. 1026\\u20131034.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b27\", \"doi-asserted-by\": \"crossref\", \"unstructured\": \"P. Abbeel, A.Y. Ng, Apprenticeship Learning via Inverse Reinforcement Learning, in: 21th International Conference on Machine Learning, 2004.\", \"DOI\": \"10.1145/1015330.1015430\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b28\", \"unstructured\": \"A.Y. Ng, S.J. Russell, et al., Algorithms for inverse reinforcement Learning, in: 17th International Conference on Machine Learning, 2000.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b29\", \"unstructured\": \"B.D. Ziebart, A.L. Maas, J.A. Bagnell, A.K. Dey, Maximum Entropy Inverse Reinforcement Learning, in: AAAI Conference on Artificial Intelligence, 2008.\"}, {\"issue\": \"2\", \"key\": \"10.1016/j.neucom.2024.128797_b30\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1056\", \"DOI\": \"10.1109/TCYB.2019.2949596\", \"article-title\": \"Task-oriented deep reinforcement learning for robotic skill acquisition and control\", \"volume\": \"51\", \"author\": \"Xiang\", \"year\": \"2021\", \"journal-title\": \"IEEE Trans. Cybern.\"}, {\"issue\": \"10\", \"key\": \"10.1016/j.neucom.2024.128797_b31\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"2030\", \"DOI\": \"10.1109/TCYB.2014.2363664\", \"article-title\": \"An integrated framework for human-robot collaborative manipulation\", \"volume\": \"45\", \"author\": \"Sheng\", \"year\": \"2015\", \"journal-title\": \"IEEE Trans. Cybern.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b32\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1155/2020/3849309\", \"article-title\": \"Learning from demonstrations and human evaluative feedbacks: Handling sparsity and imperfection using inverse reinforcement learning approach\", \"author\": \"Mourad\", \"year\": \"2020\", \"journal-title\": \"J. Robot.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b33\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"331\", \"DOI\": \"10.1016/j.eswa.2018.06.035\", \"article-title\": \"Combination of learning from non-optimal demonstrations and feedbacks using inverse reinforcement learning and Bayesian policy improvement\", \"volume\": \"112\", \"author\": \"Ezzeddine\", \"year\": \"2018\", \"journal-title\": \"Expert Syst. Appl.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b34\", \"article-title\": \"Inverse reinforcement learning in tracking control based on inverse optimal control\", \"author\": \"Xue\", \"year\": \"2021\", \"journal-title\": \"IEEE Trans. Cybern.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b35\", \"doi-asserted-by\": \"crossref\", \"unstructured\": \"G. Warnell, N. Waytowich, V. Lawhern, P. Stone, Deep Tamer: Interactive Agent Shaping in High-Dimensional State Spaces, in: AAAI Conference on Artificial Intelligence, 2018.\", \"DOI\": \"10.1609/aaai.v32i1.11485\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b36\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"66\", \"DOI\": \"10.1109/3477.979961\", \"article-title\": \"Expertness based cooperative Q-learning\", \"volume\": \"32\", \"author\": \"Ahmadabadi\", \"year\": \"2002\", \"journal-title\": \"IEEE Trans. Syst. Man Cybern. B\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b37\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1016/j.cogpsych.2020.101272\", \"article-title\": \"Individually irrational pruning is essential for ecological rationality in a social context\", \"volume\": \"118\", \"author\": \"Zendehrouh\", \"year\": \"2020\", \"journal-title\": \"Cogn. Psychol.\"}, {\"issue\": \"7\", \"key\": \"10.1016/j.neucom.2024.128797_b38\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1371/journal.pone.0103143\", \"article-title\": \"Reward maximization justifies the transition from sensory selection at childhood to sensory integration at adulthood\", \"volume\": \"9\", \"author\": \"Daee\", \"year\": \"2014\", \"journal-title\": \"PLoS One\"}, {\"issue\": \"6\", \"key\": \"10.1016/j.neucom.2024.128797_b39\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1635\", \"DOI\": \"10.1109/TNNLS.2018.2869978\", \"article-title\": \"Exploiting generalization in the subspaces for faster model-based reinforcement learning\", \"volume\": \"30\", \"author\": \"Hashemzadeh\", \"year\": \"2018\", \"journal-title\": \"IEEE Trans. Neural Netw. Learn. Syst.\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b40\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"89\", \"DOI\": \"10.1007/s12530-019-09290-9\", \"article-title\": \"Clustering subspace generalization to obtain faster reinforcement learning\", \"volume\": \"11\", \"author\": \"Hashemzadeh\", \"year\": \"2020\", \"journal-title\": \"Evol. Syst.\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b41\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"174\", \"DOI\": \"10.1109/TEVC.2021.3102241\", \"article-title\": \"Frames-of-reference-based learning: Overcoming perceptual aliasing in multistep decision-making tasks\", \"volume\": \"26\", \"author\": \"Siddique\", \"year\": \"2022\", \"journal-title\": \"IEEE Trans. Evol. Comput.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b42\", \"unstructured\": \"L. Chrisman, Reinforcement learning with perceptual aliasing: the perceptual distinctions approach, in: Proceedings Tenth National Conference on Artificial Intelligence, 1992, pp. 183\\u2013188, Cited By :174.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b43\", \"doi-asserted-by\": \"crossref\", \"unstructured\": \"P.A. Ortega, D.A. Braun, Information, Utility and Bounded Rationality, in: International Conference on Artificial General Intelligence, 2011, pp. 269\\u2013274.\", \"DOI\": \"10.1007/978-3-642-22887-2_28\"}, {\"issue\": \"2153\", \"key\": \"10.1016/j.neucom.2024.128797_b44\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1098/rspa.2012.0683\", \"article-title\": \"Thermodynamics as a theory of decision-making with information-processing costs\", \"volume\": \"469\", \"author\": \"Ortega\", \"year\": \"2013\", \"journal-title\": \"Proc. R. Soc. A: Math., Phys. Eng. Sci.\"}, {\"issue\": \"4\", \"key\": \"10.1016/j.neucom.2024.128797_b45\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1004\", \"DOI\": \"10.1016/j.neuroimage.2009.03.025\", \"article-title\": \"Bayesian model selection for group studies\", \"volume\": \"46\", \"author\": \"Stephan\", \"year\": \"2009\", \"journal-title\": \"Neuroimage\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b46\", \"first-page\": \"679\", \"article-title\": \"A Markovian decision process\", \"author\": \"Bellman\", \"year\": \"1957\", \"journal-title\": \"J. Math. Mech.\"}, {\"year\": \"2018\", \"series-title\": \"Reinforcement Learning: An Introduction\", \"author\": \"Sutton\", \"key\": \"10.1016/j.neucom.2024.128797_b47\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b48\", \"article-title\": \"Generalized Thompson sampling for sequential decision-making and causal inference\", \"volume\": \"2\", \"author\": \"Ortega\", \"year\": \"2014\", \"journal-title\": \"Complex Adapt. Syst. Model.\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b49\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1\", \"DOI\": \"10.1561/2200000070\", \"article-title\": \"A tutorial on Thompson sampling\", \"volume\": \"11\", \"author\": \"Russo\", \"year\": \"2018\", \"journal-title\": \"Found. Trends Mach. Learn.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b50\", \"unstructured\": \"Y. Gal, Z. Ghahramani, Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, in: 33rd International Conference on Machine Learning, 2016.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b51\", \"first-page\": \"1563\", \"article-title\": \"Near-optimal regret bounds for reinforcement learning\", \"volume\": \"11\", \"author\": \"Jaksch\", \"year\": \"2010\", \"journal-title\": \"J. Mach. Learn. Res.\"}, {\"issue\": \"3\\u20134\", \"key\": \"10.1016/j.neucom.2024.128797_b52\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"279\", \"DOI\": \"10.1007/BF00992698\", \"article-title\": \"Q-learning\", \"volume\": \"8\", \"author\": \"Watkins\", \"year\": \"1992\", \"journal-title\": \"Mach. Learn.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b53\", \"series-title\": \"International Conference on Artificial General Intelligence\", \"first-page\": \"343\", \"article-title\": \"Monte Carlo bias correction in Q-learning\", \"author\": \"Papadimitriou\", \"year\": \"2022\"}, {\"issue\": \"1\", \"key\": \"10.1016/j.neucom.2024.128797_b54\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"9\", \"DOI\": \"10.1007/BF00115009\", \"article-title\": \"Learning to predict by the methods of temporal differences\", \"volume\": \"3\", \"author\": \"Sutton\", \"year\": \"1988\", \"journal-title\": \"Mach. Learn.\"}, {\"year\": \"2014\", \"series-title\": \"A Developmental Method for Multimodal Sensory Integration\", \"author\": \"Daee\", \"key\": \"10.1016/j.neucom.2024.128797_b55\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b56\", \"doi-asserted-by\": \"crossref\", \"DOI\": \"10.1002/9781118445112.stat02980\", \"article-title\": \"Probability inequalities for sums of bounded random variables\", \"author\": \"Hoeffding\", \"year\": \"2014\"}, {\"year\": \"2003\", \"series-title\": \"Inequalities for the L1 Deviation of the Empirical Distribution\", \"author\": \"Weissman\", \"key\": \"10.1016/j.neucom.2024.128797_b57\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b58\", \"series-title\": \"Advances in Neural Information Processing Systems\", \"article-title\": \"Interval estimation for eeinforcement-learning algorithms in continuous-state domains\", \"author\": \"White\", \"year\": \"2010\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b59\", \"first-page\": \"4033\", \"article-title\": \"Deep exploration via bootstrapped DQN\", \"author\": \"Osband\", \"year\": \"2016\", \"journal-title\": \"Adv. Neural Inf. Process. Syst.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b60\", \"first-page\": \"5792\", \"article-title\": \"Uncertainty-aware action advising for deep reinforcement learning agents\", \"volume\": \"vol. 34\", \"author\": \"da Silva\", \"year\": \"2020\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b61\", \"series-title\": \"Perception-Action Cycle\", \"first-page\": \"601\", \"article-title\": \"Information theory of decisions and actions\", \"author\": \"Tishby\", \"year\": \"2011\"}, {\"issue\": \"4\", \"key\": \"10.1016/j.neucom.2024.128797_b62\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"375\", \"DOI\": \"10.3390/e21040375\", \"article-title\": \"Bounded rational decision-making from elementary computations that reduce uncertainty\", \"volume\": \"21\", \"author\": \"Gottwald\", \"year\": \"2019\", \"journal-title\": \"Entropy\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b63\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1292\", \"DOI\": \"10.3389/fnins.2019.01292\", \"article-title\": \"Mental effort and information-processing costs are inversely-related to global brain free energy during visual categorization\", \"volume\": \"13\", \"author\": \"Trujillo\", \"year\": \"2019\", \"journal-title\": \"Front. Neurosci.\"}, {\"year\": \"2019\", \"series-title\": \"Hierarchical expert networks for meta-learning\", \"author\": \"Hihn\", \"key\": \"10.1016/j.neucom.2024.128797_b64\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b65\", \"series-title\": \"58th Conference on Decision and Control\", \"first-page\": \"3677\", \"article-title\": \"An information-theoretic on-line learning principle for specialization in hierarchical decision-making systems\", \"author\": \"Hihn\", \"year\": \"2019\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b66\", \"doi-asserted-by\": \"crossref\", \"unstructured\": \"J. Grau-Moya, F. Leibfried, T. Genewein, D.A. Braun, Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes, in: Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2016.\", \"DOI\": \"10.1007/978-3-319-46227-1_30\"}, {\"year\": \"2003\", \"series-title\": \"Information Theory, Inference and Learning Algorithms\", \"author\": \"MacKay\", \"key\": \"10.1016/j.neucom.2024.128797_b67\"}, {\"year\": \"1989\", \"series-title\": \"Learning From Delayed Rewards\", \"author\": \"Watkins\", \"key\": \"10.1016/j.neucom.2024.128797_b68\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b69\", \"series-title\": \"Conference on Computational Intelligence and Games\", \"article-title\": \"Applying reinforcement learning to small scale combat in the real-time strategy game StarCraft: Broodwar\", \"author\": \"Wender\", \"year\": \"2012\"}, {\"issue\": \"6\", \"key\": \"10.1016/j.neucom.2024.128797_b70\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"711\", \"DOI\": \"10.1016/j.patrec.2004.01.011\", \"article-title\": \"Distance measures for PCA-based face recognition\", \"volume\": \"25\", \"author\": \"Perlibakas\", \"year\": \"2004\", \"journal-title\": \"Pattern Recognit. Lett.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b71\", \"series-title\": \"Proceedings of the 32nd International Conference on Neural Information Processing Systems\", \"first-page\": \"1856\", \"article-title\": \"Soft actor-critic algorithms and applications\", \"author\": \"Haarnoja\", \"year\": \"2018\"}, {\"year\": \"2018\", \"series-title\": \"Deepmind control suite\", \"author\": \"Tassa\", \"key\": \"10.1016/j.neucom.2024.128797_b72\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b73\", \"unstructured\": \"D. Yarats, R. Fergus, I. Kostrikov, Image augmentation is all you need: Regularizing deep reinforcement learning from pixels, in: 9th International Conference on Learning Representations, ICLR 2021, 2021.\"}, {\"year\": \"2023\", \"series-title\": \"On the importance of feature decorrelation for unsupervised representation learning in reinforcement learning\", \"author\": \"Lee\", \"key\": \"10.1016/j.neucom.2024.128797_b74\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b75\", \"doi-asserted-by\": \"crossref\", \"unstructured\": \"A. Borji, M.N. Ahmadabadi, B.N. Araabi, Learning Sequential Visual Attention Control Through Dynamic State Space Discretization, in: IEEE International Conference on Robotics and Automation, 2009.\", \"DOI\": \"10.1109/ROBOT.2009.5152543\"}, {\"issue\": \"7\", \"key\": \"10.1016/j.neucom.2024.128797_b76\", \"doi-asserted-by\": \"crossref\", \"first-page\": \"1130\", \"DOI\": \"10.1016/j.imavis.2009.10.006\", \"article-title\": \"Online learning of task-driven object-based visual attention control\", \"volume\": \"28\", \"author\": \"Borji\", \"year\": \"2010\", \"journal-title\": \"Image Vis. Comput.\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b77\", \"unstructured\": \"J. Smith, et al., Combining deep reinforcement learning and Thompson sampling for efficient exploration in complex environments, in: Proceedings of the International Conference on Machine Learning, 2019.\"}, {\"year\": \"2019\", \"series-title\": \"Efficient exploration in reinforcement learning based on thompson sampling\", \"author\": \"Chen\", \"key\": \"10.1016/j.neucom.2024.128797_b78\"}, {\"key\": \"10.1016/j.neucom.2024.128797_b79\", \"first-page\": \"1031\", \"article-title\": \"Exploration of deep reinforcement learning algorithms with thompson sampling\", \"volume\": \"67\", \"author\": \"Li\", \"year\": \"2020\", \"journal-title\": \"J. Artificial Intelligence Res.\"}], \"container-title\": [\"Neurocomputing\"], \"language\": \"en\", \"link\": [{\"URL\": \"https://api.elsevier.com/content/article/PII:S0925231224015686?httpAccept=text/xml\", \"content-type\": \"text/xml\", \"content-version\": \"vor\", \"intended-application\": \"text-mining\"}, {\"URL\": \"https://api.elsevier.com/content/article/PII:S0925231224015686?httpAccept=text/plain\", \"content-type\": \"text/plain\", \"content-version\": \"vor\", \"intended-application\": \"text-mining\"}], \"deposited\": {\"date-parts\": [[2024, 12, 2]], \"date-time\": \"2024-12-02T08:31:11Z\", \"timestamp\": 1733128271000}, \"score\": 34.92624, \"resource\": {\"primary\": {\"URL\": \"https://linkinghub.elsevier.com/retrieve/pii/S0925231224015686\"}}, \"issued\": {\"date-parts\": [[2025, 1]]}, \"references-count\": 79, \"alternative-id\": [\"S0925231224015686\"], \"URL\": \"https://doi.org/10.1016/j.neucom.2024.128797\", \"ISSN\": [\"0925-2312\"], \"issn-type\": [{\"type\": \"print\", \"value\": \"0925-2312\"}], \"published\": {\"date-parts\": [[2025, 1]]}, \"assertion\": [{\"value\": \"Elsevier\", \"name\": \"publisher\", \"label\": \"This article is maintained by\"}, {\"value\": \"Learning from different perspectives for regret reduction in reinforcement learning: A free energy approach\", \"name\": \"articletitle\", \"label\": \"Article Title\"}, {\"value\": \"Neurocomputing\", \"name\": \"journaltitle\", \"label\": \"Journal Title\"}, {\"value\": \"https://doi.org/10.1016/j.neucom.2024.128797\", \"name\": \"articlelink\", \"label\": \"CrossRef DOI link to publisher maintained version\"}, {\"value\": \"article\", \"name\": \"content_type\", \"label\": \"Content Type\"}, {\"value\": \"\\u00a9 2024 Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.\", \"name\": \"copyright\", \"label\": \"Copyright\"}], \"article-number\": \"128797\"}, {\"indexed\": {\"date-parts\": [[2025, 6, 18]], \"date-time\": \"2025-06-18T04:23:01Z\", \"timestamp\": 1750220581110, \"version\": \"3.41.0\"}, \"publisher-location\": \"New York, NY, USA\", \"reference-count\": 6, \"publisher\": \"ACM\", \"license\": [{\"start\": {\"date-parts\": [[2020, 1, 17]], \"date-time\": \"2020-01-17T00:00:00Z\", \"timestamp\": 1579219200000}, \"content-version\": \"vor\", \"delay-in-days\": 0, \"URL\": \"https://www.acm.org/publications/policies/copyright_policy#Background\"}], \"content-domain\": {\"domain\": [\"dl.acm.org\"], \"crossmark-restriction\": true}, \"published-print\": {\"date-parts\": [[2020, 1, 17]]}, \"DOI\": \"10.1145/3380688.3380706\", \"type\": \"proceedings-article\", \"created\": {\"date-parts\": [[2020, 3, 7]], \"date-time\": \"2020-03-07T12:20:13Z\", \"timestamp\": 1583583613000}, \"page\": \"51-55\", \"update-policy\": \"https://doi.org/10.1145/crossmark-policy\", \"source\": \"Crossref\", \"is-referenced-by-count\": 0, \"title\": [\"A Sublinear-Regret Reinforcement Learning Algorithm on Constrained Markov Decision Processes with reset action\"], \"prefix\": \"10.1145\", \"author\": [{\"given\": \"Takashi\", \"family\": \"Watanabe\", \"sequence\": \"first\", \"affiliation\": [{\"name\": \"Kyoto University, Graduate school of Human and Environmental Studies, Kyoto city, Japan\"}]}, {\"given\": \"Takashi\", \"family\": \"Sakuragawa\", \"sequence\": \"additional\", \"affiliation\": [{\"name\": \"Kyoto University, Graduate school of Human and Environmental Studies, Kyoto city, Japan\"}]}], \"member\": \"320\", \"published-online\": {\"date-parts\": [[2020, 3, 7]]}, \"reference\": [{\"key\": \"e_1_3_2_1_1_1\", \"series-title\": \"Vol. 7\", \"volume-title\": \"Constrained Markov decision processes\", \"author\": \"Altman Eitan\", \"unstructured\": \"Altman , Eitan . 1999. Constrained Markov decision processes ( Vol. 7 ) . CRC Press . Altman, Eitan. 1999. Constrained Markov decision processes (Vol. 7). CRC Press.\"}, {\"key\": \"e_1_3_2_1_2_1\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.2748/tmj/1178243286\"}, {\"volume-title\": \"The Collected Works of Wassily Hoeffding, 409--426.\", \"author\": \"Hoeffding Wassily\", \"key\": \"e_1_3_2_1_3_1\", \"unstructured\": \"Hoeffding , Wassily . 1994. Probability inequalities for sums of bounded random variables . In The Collected Works of Wassily Hoeffding, 409--426. , Springer , New York, NY . Hoeffding, Wassily. 1994. Probability inequalities for sums of bounded random variables. In The Collected Works of Wassily Hoeffding, 409--426., Springer, New York, NY.\"}, {\"key\": \"e_1_3_2_1_4_1\", \"volume-title\": \"Markov Decision Processes: Discrete Stochastic Dynamic Programming\", \"author\": \"Puterman\", \"unstructured\": \"Martin L. Puterman . 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming ( 1 st ed.). John Wiley & Sons, Inc. , New York, NY, USA . Martin L. Puterman. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming (1st ed.). John Wiley & Sons, Inc., New York, NY, USA.\", \"edition\": \"1\"}, {\"key\": \"e_1_3_2_1_5_1\", \"volume-title\": \"Near-optimal Regret Bounds for Reinforcement Learning. J. Mach. Learn. Res. 11 (August\", \"author\": \"Thomas Jaksch Ronald Ortner\", \"year\": \"2010\", \"unstructured\": \"Thomas Jaksch , Ronald Ortner , and Peter Auer . 2010. Near-optimal Regret Bounds for Reinforcement Learning. J. Mach. Learn. Res. 11 (August 2010 ), 1563--1600. Thomas Jaksch, Ronald Ortner, and Peter Auer. 2010. Near-optimal Regret Bounds for Reinforcement Learning. J. Mach. Learn. Res. 11 (August 2010), 1563--1600.\"}, {\"key\": \"e_1_3_2_1_6_1\", \"unstructured\": \"Tossou A. Basu D. & Dimitrakakis C. 2019. Near-optimal Optimistic Reinforcement Learning using Empirical Bernstein Inequalities. arXiv preprint arXiv:1905.12425  Tossou A. Basu D. & Dimitrakakis C. 2019. Near-optimal Optimistic Reinforcement Learning using Empirical Bernstein Inequalities. arXiv preprint arXiv:1905.12425\"}], \"event\": {\"name\": \"ICMLSC 2020: The 4th International Conference on Machine Learning and Soft Computing\", \"sponsor\": [\"NICT National Institute of Information and Communications Technology\"], \"location\": \"Haiphong City Viet Nam\", \"acronym\": \"ICMLSC 2020\"}, \"container-title\": [\"Proceedings of the 4th International Conference on Machine Learning and Soft Computing\"], \"link\": [{\"URL\": \"https://dl.acm.org/doi/10.1145/3380688.3380706\", \"content-type\": \"unspecified\", \"content-version\": \"vor\", \"intended-application\": \"text-mining\"}, {\"URL\": \"https://dl.acm.org/doi/pdf/10.1145/3380688.3380706\", \"content-type\": \"unspecified\", \"content-version\": \"vor\", \"intended-application\": \"similarity-checking\"}], \"deposited\": {\"date-parts\": [[2025, 6, 17]], \"date-time\": \"2025-06-17T21:31:32Z\", \"timestamp\": 1750195892000}, \"score\": 33.72372, \"resource\": {\"primary\": {\"URL\": \"https://dl.acm.org/doi/10.1145/3380688.3380706\"}}, \"issued\": {\"date-parts\": [[2020, 1, 17]]}, \"references-count\": 6, \"alternative-id\": [\"10.1145/3380688.3380706\", \"10.1145/3380688\"], \"URL\": \"https://doi.org/10.1145/3380688.3380706\", \"published\": {\"date-parts\": [[2020, 1, 17]]}, \"assertion\": [{\"value\": \"2020-03-07\", \"order\": 2, \"name\": \"published\", \"label\": \"Published\", \"group\": {\"name\": \"publication_history\", \"label\": \"Publication History\"}}]}, {\"indexed\": {\"date-parts\": [[2024, 9, 6]], \"date-time\": \"2024-09-06T02:25:50Z\", \"timestamp\": 1725589550393}, \"reference-count\": 0, \"publisher\": \"SCITEPRESS - Science and Technology Publications\", \"content-domain\": {\"domain\": [], \"crossmark-restriction\": false}, \"published-print\": {\"date-parts\": [[2022]]}, \"DOI\": \"10.5220/0010835600003116\", \"type\": \"proceedings-article\", \"created\": {\"date-parts\": [[2022, 2, 16]], \"date-time\": \"2022-02-16T19:49:00Z\", \"timestamp\": 1645040940000}, \"page\": \"444-453\", \"source\": \"Crossref\", \"is-referenced-by-count\": 0, \"title\": [\"Reinforcement Learning Guided by Provable Normative Compliance\"], \"prefix\": \"10.5220\", \"author\": [{\"given\": \"Emery\", \"family\": \"Neufeld\", \"sequence\": \"first\", \"affiliation\": [{\"name\": \"Faculty of Informatics, Vienna, Austria, --- Select a Country ---\"}]}], \"member\": \"3171\", \"event\": {\"name\": \"14th International Conference on Agents and Artificial Intelligence\", \"start\": {\"date-parts\": [[2022, 2, 3]]}, \"location\": \"Online Streaming, --- Select a Country ---\", \"end\": {\"date-parts\": [[2022, 2, 5]]}}, \"container-title\": [\"Proceedings of the 14th International Conference on Agents and Artificial Intelligence\"], \"original-title\": [\"Reinforcement Learning Guided by Provable Normative Compliance\"], \"deposited\": {\"date-parts\": [[2022, 7, 1]], \"date-time\": \"2022-07-01T16:51:12Z\", \"timestamp\": 1656694272000}, \"score\": 32.94955, \"resource\": {\"primary\": {\"URL\": \"https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0010835600003116\"}}, \"subtitle\": [\"\"], \"issued\": {\"date-parts\": [[2022]]}, \"references-count\": 0, \"URL\": \"https://doi.org/10.5220/0010835600003116\", \"published\": {\"date-parts\": [[2022]]}}, {\"indexed\": {\"date-parts\": [[2025, 10, 5]], \"date-time\": \"2025-10-05T16:48:30Z\", \"timestamp\": 1759682910158, \"version\": \"3.37.0\"}, \"reference-count\": 16, \"publisher\": \"Institute for Operations Research and the Management Sciences (INFORMS)\", \"issue\": \"2\", \"content-domain\": {\"domain\": [], \"crossmark-restriction\": false}, \"short-container-title\": [\"Management Science\"], \"published-print\": {\"date-parts\": [[2025, 2]]}, \"abstract\": \"<jats:p> We consider model-free reinforcement learning (RL) in nonstationary Markov decision processes. Both the reward functions and the state transition functions are allowed to vary arbitrarily over time as long as their cumulative variations do not exceed certain variation budgets. We propose Restarted Q-Learning with Upper Confidence Bounds (RestartQ-UCB), the first model-free algorithm for nonstationary RL, and show that it outperforms existing solutions in terms of dynamic regret. Specifically, RestartQ-UCB with Freedman-type bonus terms achieves a dynamic regret bound of [Formula: see text], where S and A are the numbers of states and actions, respectively, [Formula: see text] is the variation budget, H is the number of time steps per episode, and T is the total number of time steps. We further present a parameter-free algorithm named Double-Restart Q-UCB that does not require prior knowledge of the variation budget. We show that our algorithms are nearly optimal by establishing an information-theoretical lower bound of [Formula: see text], the first lower bound in nonstationary RL. Numerical experiments validate the advantages of RestartQ-UCB in terms of both cumulative rewards and computational efficiency. We demonstrate the power of our results in examples of multiagent RL and inventory control across related products. </jats:p><jats:p> This paper was accepted by Omar Besbes, revenue management and market analytics. </jats:p><jats:p> Funding: The research of D. Simchi-Levi and R. Zhu was supported by the MIT Data Science Laboratory. The research of W. Mao, K. Zhang, and T. Ba\\u015far was supported in part by the U.S. Army Research Laboratory (ARL) Cooperative Agreement W911NF-17-2-0196, in part by the Office of Naval Research (ONR) [MURI Grant N00014-16-1-2710], and in part by the Air Force Office of Scientific Research (AFOSR) [Grant FA9550-19-1-0353]. K. Zhang also acknowledges support from U.S. Army Research Laboratory (ARL) [Grant W911NF-24-1-0085]. </jats:p><jats:p> Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.02533 . </jats:p>\", \"DOI\": \"10.1287/mnsc.2022.02533\", \"type\": \"journal-article\", \"created\": {\"date-parts\": [[2024, 5, 14]], \"date-time\": \"2024-05-14T13:00:23Z\", \"timestamp\": 1715691623000}, \"page\": \"1564-1580\", \"source\": \"Crossref\", \"is-referenced-by-count\": 2, \"title\": [\"Model-Free Nonstationary Reinforcement Learning: Near-Optimal Regret and Applications in Multiagent Reinforcement Learning and Inventory Control\"], \"prefix\": \"10.1287\", \"volume\": \"71\", \"author\": [{\"ORCID\": \"https://orcid.org/0000-0001-8301-4173\", \"authenticated-orcid\": false, \"given\": \"Weichao\", \"family\": \"Mao\", \"sequence\": \"first\", \"affiliation\": [{\"name\": \"Department of Electrical and Computer Engineering & Coordinated Science Laboratory, University of Illinois Urbana-Champaign, Urbana, Illinois 61801\"}]}, {\"ORCID\": \"https://orcid.org/0000-0002-7446-7581\", \"authenticated-orcid\": false, \"given\": \"Kaiqing\", \"family\": \"Zhang\", \"sequence\": \"additional\", \"affiliation\": [{\"name\": \"Department of Electrical and Computer Engineering & Institute for Systems Research, University of Maryland, College Park, Maryland 20740\"}]}, {\"ORCID\": \"https://orcid.org/0000-0003-1463-1308\", \"authenticated-orcid\": false, \"given\": \"Ruihao\", \"family\": \"Zhu\", \"sequence\": \"additional\", \"affiliation\": [{\"name\": \"Cornell SC Johnson College of Business & Nolan School of Hotel Administration, Ithaca, New York 14853\"}]}, {\"ORCID\": \"https://orcid.org/0000-0002-4650-1519\", \"authenticated-orcid\": false, \"given\": \"David\", \"family\": \"Simchi-Levi\", \"sequence\": \"additional\", \"affiliation\": [{\"name\": \"Institute for Data, Systems, and Society, Department of Civil and Environmental Engineering, Operations Research Center, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139\"}]}, {\"ORCID\": \"https://orcid.org/0000-0003-4406-7875\", \"authenticated-orcid\": false, \"given\": \"Tamer\", \"family\": \"Ba\\u015far\", \"sequence\": \"additional\", \"affiliation\": [{\"name\": \"Department of Electrical and Computer Engineering & Coordinated Science Laboratory, University of Illinois Urbana-Champaign, Urbana, Illinois 61801\"}]}], \"member\": \"109\", \"reference\": [{\"key\": \"B4\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1007/s41060-017-0050-5\"}, {\"key\": \"B6\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1109/TAC.2016.2598476\"}, {\"key\": \"B8\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1137/S0097539701398375\"}, {\"key\": \"B11\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/mnsc.2018.3174\"}, {\"key\": \"B14\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/stsy.2019.0033\"}, {\"key\": \"B15\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/opre.2023.0363\"}, {\"key\": \"B34\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/moor.1080.0355\"}, {\"key\": \"B35\", \"first-page\": \"1563\", \"volume\": \"11\", \"author\": \"Jaksch T\", \"year\": \"2010\", \"journal-title\": \"J. Machine Learn. Res.\"}, {\"key\": \"B40\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/moor.2016.0807\"}, {\"key\": \"B46\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/moor.2017.0884\"}, {\"key\": \"B56\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/opre.2015.1474\"}, {\"key\": \"B57\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1007/s10994-010-5229-0\"}, {\"key\": \"B65\", \"unstructured\": \"Watkins CJCH (1989) Learning from delayed rewards. PhD thesis, King\\u2019s College, University of Cambridge, Cambridge, UK.\"}, {\"key\": \"B69\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/opre.2015.1446\"}, {\"key\": \"B70\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/mnsc.2020.3799\"}, {\"key\": \"B71\", \"doi-asserted-by\": \"publisher\", \"DOI\": \"10.1287/mnsc.2019.3288\"}], \"container-title\": [\"Management Science\"], \"language\": \"en\", \"link\": [{\"URL\": \"https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.2022.02533\", \"content-type\": \"unspecified\", \"content-version\": \"vor\", \"intended-application\": \"similarity-checking\"}], \"deposited\": {\"date-parts\": [[2025, 2, 12]], \"date-time\": \"2025-02-12T16:38:18Z\", \"timestamp\": 1739378298000}, \"score\": 29.050774, \"resource\": {\"primary\": {\"URL\": \"https://pubsonline.informs.org/doi/10.1287/mnsc.2022.02533\"}}, \"issued\": {\"date-parts\": [[2025, 2]]}, \"references-count\": 16, \"journal-issue\": {\"issue\": \"2\", \"published-print\": {\"date-parts\": [[2025, 2]]}}, \"alternative-id\": [\"10.1287/mnsc.2022.02533\"], \"URL\": \"https://doi.org/10.1287/mnsc.2022.02533\", \"ISSN\": [\"0025-1909\", \"1526-5501\"], \"issn-type\": [{\"type\": \"print\", \"value\": \"0025-1909\"}, {\"type\": \"electronic\", \"value\": \"1526-5501\"}], \"published\": {\"date-parts\": [[2025, 2]]}}], \"items-per-page\": 5, \"query\": {\"start-index\": 0, \"search-terms\": null}}}"}